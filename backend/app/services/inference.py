"""
Inference service for code similarity and AI detection
"""

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np
from typing import List, Dict, Tuple, Any
import logging
from pathlib import Path
from .train_detector import DualHeadCodeModel, TrainingConfig
from .advanced_ai_detector import get_advanced_detector

from .train_detector import DualHeadCodeModel, TrainingConfig

logger = logging.getLogger(__name__)

# Suppress noisy transformers warnings about uninitialized weights
# (We overwrite them immediately anyway)
logging.getLogger("transformers").setLevel(logging.ERROR)


class CodeDetectorInference:
    """Inference class for code similarity and AI detection"""
    
    def __init__(self, model_path: str, device: str = None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.device = torch.device(self.device)
        
        # Load model
        logger.info(f"Loading model from {model_path}")
        # Set weights_only=False because our checkpoint contains config and other metadata
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        config_dict = checkpoint.get('config', {})
        
        model_name = config_dict.get('model_name', 'microsoft/codebert-base')
        embedding_dim = config_dict.get('embedding_dim', 768)
        
        # Determine model type from state dict keys
        state_dict = checkpoint['model_state_dict']
        is_standard_hf = any(k.startswith('roberta.') or k.startswith('classifier.') for k in state_dict.keys())
        
        if is_standard_hf:
            logger.info("Detected standard HuggingFace Sequence Classification model")
            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_name,
                num_labels=2
            ).to(self.device)
            self.is_custom_model = False
        else:
            logger.info("Detected custom DualHeadCodeModel")
            self.model = DualHeadCodeModel(
                model_name=model_name,
                embedding_dim=embedding_dim
            ).to(self.device)
            self.is_custom_model = True
            
        self.model.load_state_dict(state_dict)
        self.model.eval()
        
        # Load tokenizer
        tokenizer_path = Path(model_path).parent / "tokenizer"
        if tokenizer_path.exists():
            self.tokenizer = AutoTokenizer.from_pretrained(str(tokenizer_path))
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        logger.info("Model loaded successfully")
    
    def preprocess_code(
        self, 
        code: str, 
        language: str = "unknown", 
        task: str = "",
        max_length: int = 512
    ) -> Dict[str, torch.Tensor]:
        """Preprocess code for inference"""
        text = f"<{language}> {code} <{task}>"
        
        encoding = self.tokenizer(
            text,
            max_length=max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].to(self.device),
            'attention_mask': encoding['attention_mask'].to(self.device)
        }
    
    def _is_boilerplate_or_simple(self, code: str) -> bool:
        """Heuristic check for extremely simple or common boilerplate code."""
        code = code.strip()
        # Very short code is hard to detect with ML, usually human or simple boilerplate
        if len(code) < 30:
            return True
        
        # Common simple statements
        simple_human_patterns = [
            'print("hello world")', "print('hello world')",
            'console.log("hello world")', "console.log('hello world')",
            'public static void main', 'int main()', 'void main()',
            'import os', 'import sys', 'import react',
            '#include <iostream>', 'using namespace std'
        ]
        
        lower_code = code.lower()
        if any(pattern in lower_code for pattern in simple_human_patterns):
            if len(code) < 100: # Only if it's relatively short
                return True
                
        return False

    def _is_suspiciously_academic(self, code: str) -> bool:
        """Catch textbook AI patterns often generated by LLMs."""
        academic_patterns = [
            'mid_idx =', 'pivot =', 'smaller_group =', 'larger_group =',
            'def quicksort', 'def fibonacci', 'def bubble_sort',
            'return organize_by_pivot'
        ]
        # Remove common "human" patterns that might be flagged
        # (none of the above are strictly common in production code, but we can be more careful)
        # For now, we'll keep these but require a higher threshold or combination
        
        lower_code = code.lower()
        # If we see 2 or more of these in one snippet, it's suspiciously like AI output
        # If we see 2 or more of these in one snippet, it's suspiciously like AI output
        matches = sum(1 for p in academic_patterns if p in lower_code)
        
        # Heuristic: If it's a very simple script but has complex algorithmic comments/structure
        if "def solution" in lower_code and "class solution" in lower_code: # Leetcode style
             matches += 1

        return matches >= 1

    @torch.no_grad()
    def detect_ai(
        self, 
        code: str, 
        language: str = "python",
        return_confidence: bool = True
    ) -> Dict[str, float]:
        """
        Detect if code is AI-generated using advanced multi-method detection
        """
        # --- ADVANCED HEURISTIC DETECTION (Primary Method) ---
        # This expert-level detector combines multiple sophisticated techniques
        advanced_detector = get_advanced_detector()
        advanced_result = advanced_detector.detect(code, language)
        
        # If confidence is high, use advanced detector result directly
        if advanced_result['confidence'] >= 0.70:  # Lowered from 0.75
            return {
                'is_ai': advanced_result['is_ai'],
                'ai_score': advanced_result['ai_score'],
                'human_score': advanced_result['human_score'],
                'confidence': advanced_result['confidence'],
                'detection_method': 'advanced_heuristics',
                'details': advanced_result.get('details', {})
            }
        
        # --- LEGACY HEURISTIC FALLBACK ---
        if self._is_boilerplate_or_simple(code):
            return {
                'is_ai': False,
                'ai_score': 0.05,
                'human_score': 0.95,
                'confidence': 0.95,
                'detection_method': 'simple_heuristic',
                'note': 'Simple/boilerplate code'
            }
            
        is_suspicious = self._is_suspiciously_academic(code)

        # --- ML DETECTION (Secondary Method) ---
        try:
            # Preprocess
            inputs = self.preprocess_code(code, language)
            
            # Forward pass
            if self.is_custom_model:
                logits, _ = self.model(
                    inputs['input_ids'],
                    inputs['attention_mask']
                )
            else:
                outputs = self.model(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask']
                )
                logits = outputs.logits
            
            # Get probabilities
            probs = F.softmax(logits, dim=1)[0]
            ml_human_score = probs[0].item()
            ml_ai_score = probs[1].item()
            
            # Ensemble: Combine ML and advanced heuristics
            # Weight advanced heuristics more (80%) since they're more reliable
            final_ai_score = (advanced_result['ai_score'] * 0.80 + ml_ai_score * 0.20)
            final_confidence = max(advanced_result['confidence'], 0.75)
            
            result = {
                'is_ai': final_ai_score >= 0.50,  # Lowered threshold
                'ai_score': final_ai_score,
                'human_score': 1 - final_ai_score,
                'confidence': final_confidence,
                'detection_method': 'ensemble',
                'details': {
                    'advanced_heuristics': advanced_result['ai_score'],
                    'ml_model': ml_ai_score,
                    **advanced_result.get('details', {})
                }
            }
            
        except Exception as e:
            logger.warning(f"ML detection failed: {e}, using advanced heuristics only")
            # Fall back to advanced heuristics result
            result = {
                'is_ai': advanced_result['is_ai'],
                'ai_score': advanced_result['ai_score'],
                'human_score': advanced_result['human_score'],
                'confidence': advanced_result['confidence'],
                'detection_method': 'advanced_heuristics_fallback',
                'details': advanced_result.get('details', {})
            }
        
        return result
    
    @torch.no_grad()
    def get_embedding(
        self, 
        code: str, 
        language: str = "python"
    ) -> np.ndarray:
        """
        Get code embedding vector
        
        Args:
            code: Source code string
            language: Programming language
        
        Returns:
            Embedding vector as numpy array
        """
        inputs = self.preprocess_code(code, language)
        
        if self.is_custom_model:
            _, embeddings = self.model(
                inputs['input_ids'],
                inputs['attention_mask'],
                return_embeddings=True
            )
            embedding = embeddings[0]
        else:
            # For standard Roberta models, we can use the pooler_output or the [CLS] token
            # Use the base model to get hidden states
            base_model = getattr(self.model, self.model.config.model_type, None)
            if base_model:
                outputs = base_model(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask']
                )
                # Use [CLS] token (first token)
                embedding = outputs.last_hidden_state[:, 0, :]
                embedding = F.normalize(embedding, p=2, dim=1)[0]
            else:
                # Fallback if base model not easily accessible
                embedding = torch.zeros(768).to(self.device)
        
        return embedding.cpu().numpy()
    
    @torch.no_grad()
    def compute_similarity(
        self, 
        code1: str, 
        code2: str,
        language1: str = "python",
        language2: str = "python"
    ) -> float:
        """
        Compute similarity between two code snippets
        
        Args:
            code1: First code snippet
            code2: Second code snippet
            language1: Language of first code
            language2: Language of second code
        
        Returns:
            Similarity score (0-1)
        """
        # Get embeddings
        emb1 = self.get_embedding(code1, language1)
        emb2 = self.get_embedding(code2, language2)
        
        # Compute cosine similarity
        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        
        return float(similarity)
    
    @torch.no_grad()
    def analyze_code_batch(
        self,
        codes: List[str],
        languages: List[str] = None
    ) -> List[Dict]:
        """
        Batch analysis of multiple code snippets
        
        Args:
            codes: List of code snippets
            languages: List of languages (optional)
        
        Returns:
            List of analysis results
        """
        if languages is None:
            languages = ["python"] * len(codes)
        
        results = []
        for code, lang in zip(codes, languages):
            ai_result = self.detect_ai(code, lang)
            embedding = self.get_embedding(code, lang)
            
            results.append({
                'code': code[:100] + "..." if len(code) > 100 else code,
                'language': lang,
                'ai_detection': ai_result,
                'embedding_norm': float(np.linalg.norm(embedding))
            })
        
        return results
    
    @torch.no_grad()
    def find_similar_submissions(
        self,
        query_code: str,
        corpus_codes: List[str],
        query_language: str = "python",
        corpus_languages: List[str] = None,
        top_k: int = 5
    ) -> List[Tuple[int, float]]:
        """
        Find top-k most similar code snippets from corpus
        
        Args:
            query_code: Query code snippet
            corpus_codes: List of corpus code snippets
            query_language: Language of query code
            corpus_languages: Languages of corpus codes
            top_k: Number of top results to return
        
        Returns:
            List of (index, similarity_score) tuples
        """
        if corpus_languages is None:
            corpus_languages = ["python"] * len(corpus_codes)
        
        # Get query embedding
        query_emb = self.get_embedding(query_code, query_language)
        
        # Get corpus embeddings
        corpus_embs = []
        for code, lang in zip(corpus_codes, corpus_languages):
            emb = self.get_embedding(code, lang)
            corpus_embs.append(emb)
        
        corpus_embs = np.array(corpus_embs)
        
        # Compute similarities
        similarities = np.dot(corpus_embs, query_emb) / (
            np.linalg.norm(corpus_embs, axis=1) * np.linalg.norm(query_emb)
        )
        
        # Get top-k
        top_indices = np.argsort(similarities)[::-1][:top_k]
        results = [(int(idx), float(similarities[idx])) for idx in top_indices]
        
        return results
    
    def comprehensive_analysis(
        self,
        code: str,
        language: str = "python",
        reference_codes: List[str] = None
    ) -> Dict:
        """
        Comprehensive analysis including AI detection and similarity
        
        Args:
            code: Code to analyze
            language: Programming language
            reference_codes: Optional list of codes to compare against
        
        Returns:
            Comprehensive analysis results
        """
        # AI detection
        ai_result = self.detect_ai(code, language)
        
        # Get embedding
        embedding = self.get_embedding(code, language)
        
        result = {
            'ai_detection': ai_result,
            'language': language,
            'embedding_dim': len(embedding),
            'code_length': len(code),
            'risk_assessment': self._assess_risk(ai_result)
        }
        
        # Similarity analysis if reference codes provided
        if reference_codes:
            similarities = []
            for ref_code in reference_codes:
                sim = self.compute_similarity(code, ref_code, language, language)
                similarities.append(float(sim))
            
            result['similarity_analysis'] = {
                'max_similarity': max(similarities) if similarities else 0.0,
                'mean_similarity': np.mean(similarities) if similarities else 0.0,
                'suspicious_pairs': sum(1 for s in similarities if s > 0.7)
            }
        
        return result
    
    def _assess_risk(self, ai_result: Dict) -> Dict:
        """Assess overall risk level"""
        ai_score = ai_result['ai_score']
        
        if ai_score >= 0.8:
            level = "critical"
            description = "Very high likelihood of AI-generated code"
        elif ai_score >= 0.6:
            level = "high"
            description = "High likelihood of AI-generated code"
        elif ai_score >= 0.4:
            level = "medium"
            description = "Moderate likelihood of AI-generated code"
        else:
            level = "low"
            description = "Likely human-written code"
        
        return {
            'level': level,
            'description': description,
            'confidence': ai_result['confidence']
        }


# Global inference instance (lazy loading)
_detector_instance = None


def get_detector(model_path: str = None) -> CodeDetectorInference:
    """Get or create detector instance"""
    global _detector_instance
    
    if _detector_instance is None:
        if model_path is None:
            # Check for models in priority order:
            # 1. Fine-tuned model (best)
            # 2. Latest AUC model
            # 3. Demo model (fallback)
            from pathlib import Path
            model_dir = Path("app/services/models/code_detector")
            
            fine_tuned_path = model_dir / "fine_tuned_codebert.pt"
            best_auc_path = model_dir / "best_model_auc_1.0000.pt" # Example path
            demo_path = model_dir / "demo_model.pt"
            
            # Find the best available model
            if fine_tuned_path.exists():
                model_path = str(fine_tuned_path)
                logger.info("Using fine-tuned CodeBERT model")
            else:
                # Look for any .pt files starting with 'best_model'
                best_models = list(model_dir.glob("best_model_auc_*.pt"))
                if best_models:
                    # Sort by AUC in filename (highest last)
                    best_models.sort()
                    model_path = str(best_models[-1])
                    logger.info(f"Using trained model: {model_path}")
                elif demo_path.exists():
                    model_path = str(demo_path)
                    logger.info("Using demo model (fallback)")
                else:
                    raise FileNotFoundError("No model found. Run fine_tune_model.py first.")
        
        _detector_instance = CodeDetectorInference(model_path)
        # Tag it if it's the demo model
        if "demo_model" in str(model_path):
            _detector_instance.is_demo = True
        else:
            _detector_instance.is_demo = False
    
    return _detector_instance


def detect_ai_generated(code: str, language: str = "python") -> Dict:
    """Convenience function for AI detection"""
    detector = get_detector()
    return detector.detect_ai(code, language)


def compute_code_similarity(code1: str, code2: str) -> float:
    """Convenience function for similarity computation"""
    detector = get_detector()
    return detector.compute_similarity(code1, code2)
